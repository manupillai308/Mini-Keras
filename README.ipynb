{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Activation Function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Every new implementation of Activation function should be a class containing 3 methods,\r\n",
    "\r\n",
    "- ```__call__``` : returns ```eval```\r\n",
    "- ```eval``` : Method that returns the output of the activation function for an input X of shape (features, batch_size).\r\n",
    "- ```grad_input``` : Method that returns the Jacobian the activation function with respect to the input of the function. \r\n",
    "For an activation function $f = [f_1, f_2, ..., f_n]$ with input $x = [x_1, x_2, ..., x_n]$, the ```grad_input``` returns \r\n",
    "$\\begin{bmatrix}\r\n",
    "    \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\ldots & \\frac{\\partial f_1}{\\partial x_n} \\\\\r\n",
    "    \\vdots & & \\ddots & \\vdots \\\\\r\n",
    "    \\frac{\\partial f_n}{\\partial x_1} & \\frac{\\partial f_n}{\\partial x_2} & ... & \\frac{\\partial f_n}{\\partial x_n} \\\\\r\n",
    "  \\end{bmatrix}$\r\n",
    "\r\n",
    "It should be noted that all the implementations must be using numpy and loops are required to be avoided at most of the places. \r\n",
    "\r\n",
    "For an example, check ```activation.Sigmoid``` class in the repository.\r\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}